# On Multivariate Goodness–of–Fit and Two–Sample Testing
### *Introduction*
Let $$N$$ independent observations $$\{x_{i}\}_{i=1}^{N}$$ from whatever probability distribution $$p(x)$$, the Goodness-of-Fit problem is check the validity of the hyphotesis $$p(x)$$ = $$p_{0}(x)$$, where $$p_{0}(x)$$ is a specific distribution probability.  
Related to this testing problem we have: let $$\{x_{i}\}_{i=1}^{N}$$ a data-set drawn from a distribution $$p(x)$$ and $$\{z_{i}\}_{i=1}^{M}$$ drawn from a distribution with density $$q(z)$$, the Two-Sample-Test problem is to test the hypothesis $$H_{0}$$ : $$p(x)$$ = $$q(z)$$ against all the alternatives. As $$N\to\infty$$ and $$M\to\infty$$ the test will reject the null hypothesis when $$H_{1}$$ is true,  $$p$$ $$\neq$$ $$q$$.
Although in univariate case, where each observation $$x_{i}$$ or $$z_{i}$$ is a single measurement, there are many two-sample-testing we can perform, in high dimensionality setting where each observation is composed by many measurements $$x_{k}$$ = $$\{x_{k1},x_{k2},\cdots,x_{kn}\}$$, these tests lose their power as $${n}$$ increases.
 
### Machine learning & Two-Sample Testing
Classification procedures can be applied to two sample testing. The input or predictor variable are created by pooling the two samples together: $$\{u_{i}\}_{i=1}^{N+M}$$ = $$\{x_{i}\}_{i=1}^{N}$$ $$\cup$$ $$\{z_{i}\}_{i=1}^{M}$$ and at each observation is assigned a response value $$\{y_{i}\}$$ = $$1$$ if it comes from the first sample ($$1$$ $$\leq$$ $$i$$ $$\leq$$ $$N$$), otherwise $$\{y_{i}\}$$ = $$0$$ if it comes from the second sample ( $$N$$ $$+$$ $$1$$ $$\leq$$ $$i$$ $$\leq$$ $$N$$ $$+$$ $$M$$). Then a classification algorithm is performed over the dataset producing a score function $$F(\bold{u})$$ and according to it all observations are scored $$\{s_{i}$$ = $$F(u_{i})\}_{i=1}^{N+M}$$.
Let $$S_{1}$$ $$=$$ $$\{s_{i}\}_{i=1}^{N}$$ and $$S_{0}$$ $$=$$ $$\{s_{i}\}_{i=N+1}^{N+M}$$ be the sets of scores respectively assigned to the two samples $$\{x_{i}\}_{i=1}^{N}$$ and $$\{z_{i}\}_{i=1}^{M}$$, these sets can be viewed like two random samples from two distributions with densities $$p_{1}(s)$$ and $$p_{0}(s)$$. Now we can consider a univariate two-sample testing where $$H_{0}$$ : $$p_{1}(s)$$ $$=$$ $$p_{0}(s)$$ and $$\hat{t}$$ $$=$$ $$T(\{s_{i}\}_{i=1}^{N},\{s_{i}\}_{i=N+1}^{N+M})$$ is the value of test statistic like Kolmogorov-Smirnov or Mann-Whitney. The output value of these test statistic will be the value for the *multivariate* two-sample testing ($$H_{0}$$ : $$p$$ $$=$$ $$q$$). 

####  Why auxiliary tests?
In  statistical hypothesis testing field, auxiliary tests are adopted to check an assumption and the validity of a primary test, such as: normality, independence, shape, ecc... .
In the "Multivariate Goodness–of–Fit and Two–Sample Testing" scenario, Kolmogorov-Smirnov or Mann-Whitney are usually performed as auxiliary test, in order to check the validity of the result of the machine learning algotithm, which classifies the observations of the two sample sizes according to the score function $$F(
\bold{u})$$. They are often preferred to others (i.e. t-test or chi-squared), since no assumption about the underlying distributions of the samples tested is neeed, indeed both of them belong to the family of non-parametric tests, a wide range of statistical tests based on no assumption on distributions or with unspecified parameters.


### Null distribution $$H_{0}(t)$$
If the goal is testing the null hypothesis $$p(x)$$ = $$q(x)$$, we need to know the distribution, under the null, $$H_{0}(t)$$. It will reject the null $$H_{0}$$ at significant level $$1$$ - $$\alpha$$ if the statistical value $$\hat{t}$$ is greater than or equal to  the $$1$$ - $$\alpha$$ qauntile of the distribution $$H_{0}(t)$$. This distribution is valid when separated independent datasets are used respectively for training and evaluating the scores, otherwise we can perform a permutation test: basically the actually response values $$\{y_{i}\}_{i=1}^{N+M}$$ are randomly permuted among all the observations $$\{u_{i}\}_{i=1}^{N+M}$$ and learning, scoring data and the test statistic are now performed over the permuted dataset.
This random permutation is done many times (M) and a set of test statistic results is collected $$\{t_{i}\}_{i=1}^{M}$$ and we reject the null hypothesis at significant level $$1$$ - $$\alpha$$ if the value of statistic test $$\hat{t}$$ (original dataset) is greater than or equal to $$1$$ - $$\alpha$$ quantile of $$\{t_{i}\}_{i=1}^{M}$$ distribution. 


### From Two-Sample-Testing to Goodness-of-Fit
As we said, in the goodness-of fit testing problem we draw a Monte Carlo random sample of size $$M$$ from the reference distribution $$F_{0}(x)$$ and then we test it with the data sample $$\{x_{i}\}_{i=1}^{N}$$, from an unknown distributio  $$F(x)$$, in order to check the validity of this hypothesis $$F(x)$$ = $$F_{0}(x)$$. In additiion the power of this test will increase as $$M$$ increases.
The permuation  procedure of two-sample-tetsing scenario is still valid, but there is another method to construct the null distribution, increasing the power.
First of all one draws Monte Carlo random sample of size $$M$$ from $$F_{0}(x)$$ $$P$$ times; each sample $$\{z_{i}\}_{i=1}^{M}$$ is combined with the data sample $$\{x_{i}\}_{i=1}^{N}$$ and the learning machine is trained and all the observations are scored acccording to the score function $$F(u)$$. Then a test statistic is computed over the scores, producing a value $$\hat{t}$$. Repeating this process $$P$$ times it produces a set of test statistics $$\{\hat{t}_{i}\}_{i=1}^{P}$$ and it is used to test the null hypothesis as always.
