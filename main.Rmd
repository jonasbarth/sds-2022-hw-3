---
title: "main"
author: "Jonas Barth, Mattia Castaldo, Matteo Migliarini"
date: "2023-02-03"
output: html_document
---

# Index
- [Simulation Study](#simulation-study)
    - [Setup](#parameter-setup)
    - [Creating Data](#creating-data)
    

# Setup
```{r setup, include=FALSE}
load("hw3_data.RData")
library('dplyr')
library('pROC')
library('transport')
library('boot')
library('InformationValue')
library('glmnet') # for regularising our model
# Parallelization libraries
library(foreach)
library(iterators)
library(doParallel) 
registerDoParallel(cores = detectCores())
```


# Simulation Study {#simulation-study}
Repeat $M$ times:

1. pick two k-variate distributions to sample from. Each distribution represents one class.
1. sample $n_0$ and $n_1$ number of samples from the respective distributions.
1. train a binary classifier
1. do permutation test (slide 8)
Experiment with different:

distributions
sample sizes
Finally, summarise results.

## Setting up parameters {#parameter-setup}
We set up the following parameters for the **simulation study**. We will sample from an **exponential** and from a **normal** distribution, which require the parameters $\labmda, \mu, \sigma$ to be set. We also define the simulation size $M$, the number of dimensions $k$ for each distribution, and the number of samples from each distribution, $n0$ and $n1$ respectively.
```{r hyperparameters}
M = 1e3
P = 10

k = 5
n0 = 100
n1 = 150

lambda = 0.5
mu = 1 / lambda
sigma = mu
```

## Creating Data {#creating-data}
```{r}
random.distro <- function(n, mean = 1, sd = 1, noisiness = sd * 0.5, eps = 1e-5) {
  #' Generates a univariate random distribution with n samples.
  rd <- sample(1:5, 1)[[1]]
  #print(rd)

  if (rd==1) {
    rate = 1 / sd
    X = rexp(n,rate) + (mean - rate)
  } else if (rd ==2) {
    X = rnorm(n, mean, sd)
  } else if (rd == 3) {
    shape = (mean/sd)^2
    scale = sd / sqrt(shape)
    X = rgamma(n, shape, scale = scale)
#  } else if (rd == 4) {
#    mean = min(abs(mean), 1-eps)
#    nu = mean * (1-mean) / sd^2 - 1
#    alpha = mean * nu
#    beta  = (1-mean) * nu
#    X = rbeta(n, alpha, beta)
  } else if (rd == 4) {
    sd = max(sqrt(2) + eps, sd)
    df = 2 * sd^2 / (sd^2 - 1)
    X = rt(n, df) + mean
  } else {
    b = mean + sd / sqrt(2)
    a = mean - sd / sqrt(2)
    X =  runif(n, a, b)
  }
  
  noise = rnorm(n, 0, noisiness)
  return(X + noise)
}

random.data <- function(n, k, means = rnorm(k, 0, 3), sds = rnorm(k, 3, 1)) {
  #' Generates a single k-variate random distribution with n samples.
  cols = list()
  for (i in 1:k) {
    mean = means[i]
    sd = max(1, sds[i])
    cols[[paste(i)]] = random.distro(n, mean, sd)
  }
  return(data.frame(cols))
}

random.data(10,10)
```


```{r}
means = rnorm(5, 0,3)
sds   = rnorm(5, 3, 1)

X = random.data(100, 5, means = means, sds = sds)
Y = random.data(100, 5, means = means, sds = sds)

wasserstein(pp(X), pp(Y))
#subwasserstein(pp(X), pp(Y), 30, 100)

#sample(X, 10)
```


```{r}
create_df = function(n0, n1, k, H0 = F, shuffle = T) {
    #' Creates a dataframe with samples from the two distributions.
    #' 
    #' @param n0 number of samples from dist0
    #' @param n1 number of samples from dist1
    #' @param k  number of features
    #' @param H0 
    #' 
    #' @return a dataframe with n0 samples from dist0, n1 samples from dist1, and a label column.
    
    means = rnorm(k, 0, 10)
    sds   = rnorm(k, 3, 1)
    if (H0) {
      X  = random.data(n0+n1, k, means, sds)
      X0 = X[1:n0, ]
      X1 = X[(n0+1):(n0+n1), ]
    } else {
      X0 = random.data(n0, k, means, sds)
      X1 = random.data(n1, k, means, sds)
    }
    
    y0 = rep(0, n0)
    y1 = rep(1, n1)
    y = rep(c(0, 1), c(n0, n1))
    dim(y) = c(n0 + n1, 1)
    
    n.min = min(n0, n1)
    X0.sample = X0[sample(nrow(X0), n.min), ]
    X1.sample = X1[sample(nrow(X1), n.min), ]
    d = wasserstein(pp(X0.sample), pp(X1.sample))

    X = rbind(X0, X1)
    df = data.frame(X)
    df$label = y
    
    if (shuffle)
      df = df[sample(nrow(df), nrow(df)), ]
    
    # Train Test split
    sample <- sample(c(TRUE, FALSE), nrow(df), replace=TRUE, prob=c(0.7,0.3))
    df.train  <- df[sample, ]
    df.test   <- df[!sample, ]
    
    generated = list()
    generated$df.train = df.train
    generated$df.test  = df.test
    generated$df = df
    generated$distance = d
    generated$means = means
    generated$sds = sds
    generated$same.src = H0
    generated$X = X
    generated$y = y
    
    return(generated)
}
```



## Simulation Loop {#simulation-loop}
```{r simulation loop}
alpha = 0.05
tests = integer(M)
dists = numeric(M)
results = vector(length = M)

simulate = function(M, n0, n1, k, create.data, H0) {
    #' Simulates operating characteristics of a testing procedure for two distributions.
    #' @param M the number of simulations
    #' @param n0 the number of samples to draw from the first distribution.
    #' @param n1 the number of samples to draw from the second distribution.
    #' @param k the number of features in each distribution.
    #' @param create.data a function for creating a dataframe containing n0 + n1 samples, from the two distributions. The function should take the parameters n0, n1, k.
    #' @return a result vector with p-values from the testing procedure.
    
    results = vector(length = M)
    

    for (i in 1:M) {
        generated = create.data(n0, n1, k, H0 = H0)
        df = generated$df
        df.train = generated$df.train
        df.test  = generated$df.test
        
        distance = generated$distance

        model = glm(formula=label ~ ., data=df.train, family=binomial)
        
        X.test = df.test[,1:k]
        y_pred = predict(model, df.test, type='response')
        df.test$y_pred = y_pred
        y_pred_class_1 = df.test[df.test$label == 1,"y_pred"]
        y_pred_class_0 = df.test[df.test$label == 0,"y_pred"]

        result = wilcox.test(y_pred_class_0, y_pred_class_1)
        
        results[i] = result$p.value
    }
    return(results)
}
```

```{r warning=FALSE}
create.normal.df = function(n0, n1, k) {
    #' Creating a dataframe with samples from the same normal distribution.
    X0 = rnorm(n0 * k, mean = 2, sd = 7)
    dim(X0) = c(n0, k)
    y0 = rep(0, n0)
    
    X1 = rnorm(n1 * k, mean = 2, sd = 7)
    dim(X1) = c(n1, k)
    y1 = rep(1, n1)
    
    y = rep(c(0, 1), c(n0, n1))
    dim(y) = c(n0 + n1, 1)
    
    
    n.min = min(n0, n1)
    X0.sample = X0[sample(nrow(X0), n.min), ]
    X1.sample = X1[sample(nrow(X1), n.min), ]
    d = wasserstein(pp(X0.sample), pp(X1.sample))

    X = rbind(X0, X1)
    df = data.frame(X)
    df$label = y
    df = df[sample(nrow(df), nrow(df)), ]
    
    # Train Test split
    sample <- sample(c(TRUE, FALSE), nrow(df), replace=TRUE, prob=c(0.7,0.3))
    df.train  <- df[sample, ]
    df.test   <- df[!sample, ]
    
    generated = list()
    generated$df.train = df.train
    generated$df.test  = df.test
    generated$df = df
    generated$distance = d
    
    return(generated)
}

```



Sampling from the same distribution still produces many low p-values, meaning that we reject the null-hypothesis many times. This means that we think that the classes come from two **different** distributions even though they come from the same. This is even though the scores produced by the classifier appear to be similar as shown in the histogram below.
```{r H1 simulation, warning=FALSE}
results.H1 = simulate(M, n0, n1, k, create_df, H0=FALSE)
hist(results.H1, breaks = 20)
abline(v=alpha, col = 'orange', lwd=3)
```

```{r H0 simulation, warning=FALSE}
results.H0 = simulate(M, 200, 200, k, create_df, H0=TRUE)
hist(results.H0, breaks = 20)
abline(v=alpha, col = 'orange', lwd=3)
```



```{r params simulation, warning=FALSE}
k.values = seq(3, 7, by=2)
N.values = seq(300, 900, by = 300)
prop.values = c(0.2, 0.5)

total_params = length(k.values) * length(N.values) * length(prop.values)
params.df = expand.grid(k=k.values, N=N.values, prop=prop.values)
params.df$it = 1:total_params
params.df$size  = numeric(total_params)
params.df$power = numeric(total_params)

for (i in 1:nrow(params.df)) {
  row = params.df[i, ]
  n0 = as.integer(row$N * row$prop)
  n1 = as.integer(row$N - n0)
  
  row$size = sum(simulate(M, n0, n1, row$k, create_df, H0=TRUE) < 0.05) / M
  row$power = sum(simulate(M, n0, n1, row$k, create_df, H0=FALSE) > 0.05)/M
}

```

```{r simulation parallelized, warning=FALSE}
res = foreach(k=params.df$k, N=params.df$N, prop=params.df$prop, .combine=rbind) %dopar% {
    n0 = as.integer(N * prop)
    n1 = as.integer(N - n0)
    
    size = sum(simulate(M, n0, n1, k, create_df, H0=TRUE) < 0.05) / M
    power = sum(simulate(M, n0, n1, k, create_df, H0=FALSE) > 0.05)/M
    
    data.frame(k=k, N=N, prop=prop, size=size, power=power)
}
params.df
```







# 1. Load and pool {#load-and-pool-data}
First we normalise data coming from different labs, as the caltech lab and trinity lab have a very different scale. By doing this we hope to mitigate some negative effects on the pooling when later we'll try to use the mean to summarize the values across subjects or time, as a very different scale would have highly impacted the mean values.

```{r Correct lab effect}
asd = bind_rows(asd_sel, .id = 'id')
asd$src = 'asd'
td  = bind_rows(td_sel, .id = 'id')
td$src  = 'td'
td$lab  = ifelse(grepl('trinity', td$id, fixed = TRUE), 'trinity', 'caltech')
asd$lab = ifelse(grepl('trinity', asd$id, fixed = TRUE), 'trinity', 'caltech')

all = rbind(asd, td)

all[all$lab=='caltech',2:117] = scale(all[all$lab=='caltech',2:117])
all[all$lab=='trinity',2:117] = scale(all[all$lab=='trinity',2:117])


asd = all[all$src == 'asd', 2:117]
td  = all[all$src == 'td', 2:117]
```

## Pooling

We tried pooling data together in different manners, by mean or by median, across subjects or across time.

Pool data together by mean across subjects:
```{r mean per subject}
apply_per_subject <- function(df, func, subject_num = 12, jump = 145) {
  df.pool = data.frame()
  for(i in 1:jump) {
    el = apply(df[seq(i, by=jump, length.out=subject_num), ],2, func)
    df.pool = rbind(df.pool, el)
  }
  colnames(df.pool) = colnames(df)
  return(df.pool)
}

asd.pool = apply_per_subject(asd, mean)
td.pool  = apply_per_subject(td, mean)
all.pool = rbind(asd.pool,td.pool)
```

Pool data together by median across subjects:
```{r median per subject}
asd.pool = apply_per_subject(asd, median)
td.pool  = apply_per_subject(td, median)
all.pool = rbind(asd.pool,td.pool)
```

Pool data together by mean across time:
```{r mean per time, eval=FALSE}
apply_per_time <- function(df, func, timestamps = 145, subjects_num = 12) {
  df.pool = data.frame()
  for(i in 0:11) {
    el = apply(df[seq(1+i*timestamps, length.out=timestamps), ],2, func)
    df.pool = rbind(df.pool, el)
  }
  colnames(df.pool) = colnames(df)
  return(df.pool)
}

asd.pool = apply_per_time(asd, mean)
td.pool  = apply_per_time(td, mean)
```

Pool data together by median across time:
```{r median per time, eval=FALSE}
asd.pool = apply_per_time(asd, median)
td.pool  = apply_per_time(td, median)
```

We noticed that the pooling that works the best is the one using **median across subjects**, since it's the only one that gives us a decent amount of correlations.
