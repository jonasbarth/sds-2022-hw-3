---
title: "main"
author: "Jonas Barth, Mattia Castaldo, Matteo Migliarini"
date: "2023-02-03"
output: html_document
---

# Index
- [Simulation Study](#simulation-study)
    - [Setup](#parameter-setup)
    - [Creating Data](#creating-data)
    

# Setup
```{r include=FALSE}
install.packages("dplyr")
install.packages("pROC")
install.packages("transport")
install.packages("glmnet")
install.packages("doParallel")
install.packages("foreach")
install.packages("iterators")
install.packages("doParallel")
install.packages("ggplot2")
```


```{r setup, include=FALSE}
load("hw3_data.RData")
library('dplyr')
library('pROC')
library('transport')
library('boot')
library('ggplot2')
library('InformationValue')
library('glmnet') # for regularising our model
# Parallelization libraries
library(foreach)
library(iterators)
library(doParallel) 
registerDoParallel(cores = detectCores())
```


# Simulation Study {#simulation-study}
Repeat $M$ times:

1. pick two k-variate distributions to sample from. Each distribution represents one class.
1. sample $n_0$ and $n_1$ number of samples from the respective distributions.
1. train a binary classifier
1. do permutation test (slide 8)
Experiment with different:

distributions
sample sizes
Finally, summarise results.

## Setting up parameters {#parameter-setup}
We set up the following parameters for the **simulation study**. We will sample from an **exponential** and from a **normal** distribution, which require the parameters $\labmda, \mu, \sigma$ to be set. We also define the simulation size $M$, the number of dimensions $k$ for each distribution, and the number of samples from each distribution, $n0$ and $n1$ respectively.
```{r hyperparameters}
M = 1e3
P = 10

k = 5
n0 = 100
n1 = 150

lambda = 0.5
mu = 1 / lambda
sigma = mu
```

## Creating Data {#creating-data}
```{r}
random.distro <- function(n, mean = 1, sd = 1, noisiness = sd * 0.5, eps = 1e-5) {
  #' Generates a univariate random distribution with n samples.
  rd <- sample(1:5, 1)[[1]]
  #print(rd)

  if (rd==1) {
    rate = 1 / sd
    X = rexp(n,rate) + (mean - rate)
  } else if (rd ==2) {
    X = rnorm(n, mean, sd)
  } else if (rd == 3) {
    shape = (mean/sd)^2
    scale = sd / sqrt(shape)
    X = rgamma(n, shape, scale = scale)
#  } else if (rd == 4) {
#    mean = min(abs(mean), 1-eps)
#    nu = mean * (1-mean) / sd^2 - 1
#    alpha = mean * nu
#    beta  = (1-mean) * nu
#    X = rbeta(n, alpha, beta)
  } else if (rd == 4) {
    sd = max(sqrt(2) + eps, sd)
    df = 2 * sd^2 / (sd^2 - 1)
    X = rt(n, df) + mean
  } else {
    b = mean + sd / sqrt(2)
    a = mean - sd / sqrt(2)
    X =  runif(n, a, b)
  }
  
  noise = rnorm(n, 0, noisiness)
  return(X + noise)
}

random.data <- function(n, k, means = rnorm(k, 0, 3), sds = rnorm(k, 3, 1)) {
  #' Generates a single k-variate random distribution with n samples.
  cols = list()
  for (i in 1:k) {
    mean = means[i]
    sd = max(1, sds[i])
    cols[[paste(i)]] = random.distro(n, mean, sd)
  }
  return(data.frame(cols))
}

random.data(10,10)
```


```{r}
beg <- Sys.time()
for (i in 1: 100) {
  means = rnorm(7, 0,3)
  sds   = rnorm(7, 3, 1)
  
  X = random.data(450, 7, means = means, sds = sds)
  Y = random.data(450, 7, means = means, sds = sds)
  
  #wasserstein(pp(X), pp(Y))
  subwasserstein(pp(X), pp(Y), 30, 100)
}
fin<- Sys.time() - beg
fin
#sample(X, 10)
```


```{r}
create_df = function(n0, n1, k, H0 = F, shuffle = T, distance = FALSE) {
    #' Creates a dataframe with samples from the two distributions.
    #' 
    #' @param n0 number of samples from dist0
    #' @param n1 number of samples from dist1
    #' @param k  number of features
    #' @param H0 
    #' 
    #' @return a dataframe with n0 samples from dist0, n1 samples from dist1, and a label column.
    
    means = rnorm(k, 0, 10)
    sds   = rnorm(k, 3, 1)
    if (H0) {
      X  = random.data(n0+n1, k, means, sds)
      X0 = X[1:n0, ]
      X1 = X[(n0+1):(n0+n1), ]
    } else {
      X0 = random.data(n0, k, means, sds)
      X1 = random.data(n1, k, means, sds)
    }
    
    y0 = rep(0, n0)
    y1 = rep(1, n1)
    y = rep(c(0, 1), c(n0, n1))
    dim(y) = c(n0 + n1, 1)
    
    n.min = min(n0, n1)
    X0.sample = X0[sample(nrow(X0), n.min), ]
    X1.sample = X1[sample(nrow(X1), n.min), ]

    X = rbind(X0, X1)
    df = data.frame(X)
    df$label = y
    
    if (shuffle)
      df = df[sample(nrow(df), nrow(df)), ]
    
    # Train Test split
    sample <- sample(c(TRUE, FALSE), nrow(df), replace=TRUE, prob=c(0.7,0.3))
    df.train  <- df[sample, ]
    df.test   <- df[!sample, ]
    
    generated = list()
    generated$df.train = df.train
    generated$df.test  = df.test
    generated$df = df
    generated$means = means
    generated$sds = sds
    generated$same.src = H0
    generated$X = X
    generated$y = y
    
    if (distance)
      generated$distance = wasserstein(pp(X0.sample), pp(X1.sample))
    
    return(generated)
}
```



## Simulation Loop {#simulation-loop}
```{r simulation loop}
alpha = 0.05
tests = integer(M)
dists = numeric(M)
results = vector(length = M)

simulate = function(M, n0, n1, k, create.data, H0) {
    #' Simulates operating characteristics of a testing procedure for two distributions.
    #' @param M the number of simulations
    #' @param n0 the number of samples to draw from the first distribution.
    #' @param n1 the number of samples to draw from the second distribution.
    #' @param k the number of features in each distribution.
    #' @param create.data a function for creating a dataframe containing n0 + n1 samples, from the two distributions. The function should take the parameters n0, n1, k.
    #' @return a result vector with p-values from the testing procedure.
    
    results = vector(length = M)
    

    for (i in 1:M) {
        generated = create.data(n0, n1, k, H0 = H0)
        df = generated$df
        df.train = generated$df.train
        df.test  = generated$df.test
        
        distance = generated$distance

        model = glm(formula=label ~ ., data=df.train, family=binomial)
        
        X.test = df.test[,1:k]
        y_pred = predict(model, df.test, type='response')
        df.test$y_pred = y_pred
        y_pred_class_1 = df.test[df.test$label == 1,"y_pred"]
        y_pred_class_0 = df.test[df.test$label == 0,"y_pred"]

        result = wilcox.test(y_pred_class_0, y_pred_class_1)
        
        results[i] = result$p.value
    }
    return(results)
}
```

We can observe that when the null hypothesis is false then the distribution ov p-values is highly skewed towards 0. In particular we can observe that the p-value is almost always under the $\alpha = 0.05$ threshold.
```{r H1 simulation, echo=FALSE, warning=FALSE}
results.H1 = simulate(M, n0, n1, k, create_df, H0=FALSE)
hist(results.H1, 
     breaks = 20,
     main='Distribution of p-values when H1 is True',
     xlab='p-value')
abline(v=alpha, col = 'orange', lwd=3)
text(alpha + 0.03, 800, labels = 'α', col='orange')
```

We observe that when the null hypothesis is True the distribution of p-values is almost uniform, as expected.
```{r H0 simulation, echo=FALSE, warning=FALSE}
results.H0 = simulate(M, n0, n1, k, create_df, H0=TRUE)
hist(results.H0, 
     breaks = 20,
     main='Distribution of p-values when H0 is True',
     xlab='p-value')
abline(v=alpha, col = 'orange', lwd=3)
text(alpha + 0.03, 55, labels= 'α', col='orange')
```

## Experiment with different parameters

Now we're going to try the same test but with different parameters:
- the number of features per sample: $k \in \{3, 5, 7\}$
- the number of samples in total: $N \in \{300, 600, 900\}$
- whether the data classes are balance or unbalanced, $p$ being the proportion of one class over the total: $p \in \{20\% , 50\%\} $


```{r experiment new params simulation, eval=FALSE, warning=FALSE}
k.values = seq(3, 7)
N.values = seq(300, 2000, by = 100)
prop.values = c(0.2, 0.5)

total_params = length(k.values) * length(N.values) * length(prop.values)
params.df = expand.grid(k=k.values, N=N.values, prop=prop.values)

performance = data.frame()

for (i in 1:nrow(params.df)) {
  row = params.df[i, ]
  n0 = as.integer(row$N * row$prop)
  n1 = as.integer(row$N - n0)
  
  performance = rbind(
    data.frame(
      N    = rep.int(row$N, M),
      prop = rep(row$prop,  M),
      k    = rep.int(row$k, M),
      size = simulate(M, n0, n1, row$k, create_df, H0=TRUE),
      power= simulate(M, n0, n1, row$k, create_df, H0=FALSE)
    ),
    performance
  )
  write.csv(performance, 'performance.csv', row.names = FALSE) 
}

```

```{r load performance data from disk, include=FALSE}
performance = read.csv('performance.csv')

k.values = sort(unique(performance$k))
N.values = sort(unique(performance$N))
prop.values = sort(unique(performance$prop))
params.df = expand.grid(k=k.values, N=N.values, prop=prop.values)
```

```{r}
performance$prop = ifelse(performance$prop == 0.2, 'Imbalanced 20-80', 'Balanced 50-50')

performance.summary = performance |>
  group_by(k, N, prop) |>
  dplyr::summarise(
    type.one = sum(size < alpha) / length(size),
    type.two = 1 - sum(power > alpha) / length(power), 
    .groups = 'keep'
  )

performance.summary |>
  ggplot(
    aes(x=k, y = type.one)
  ) +
  geom_boxplot(aes(group=interaction(k, prop), col=prop)) +
  geom_dotplot(aes(group=k+N, fill=N), binaxis='y', stackdir='center', dotsize=.2, binwidth = 0.005) +
  ggtitle('Distribution of Type I error probability') +
  xlab('Number of features [k]') +
  ylab('P(Type I error)') + 
  labs(fill = 'Number of samples', col = 'Class balance')
```

```{r}
performance.summary |>
  ggplot(
    aes(x=k, y = type.two)
  ) +
  geom_boxplot(aes(group=interaction(k, prop), col=prop)) +
  geom_dotplot(aes(group=k+N, fill=N), binaxis='y', stackdir='center', dotsize=.5, binwidth = 0.01) +
  ggtitle('Distribution of Type II error probability') +
  xlab('Number of features [k]') +
  ylab('1 - P(Type II error)') + 
  labs(fill = 'Number of samples', col = 'Class balance')
```





# 1. Load and pool {#load-and-pool-data}
First we normalise data coming from different labs, as the caltech lab and trinity lab have a very different scale. By doing this we hope to mitigate some negative effects on the pooling when later we'll try to use the mean to summarize the values across subjects or time, as a very different scale would have highly impacted the mean values.

```{r Correct lab effect, eval=FALSE}
asd = bind_rows(asd_sel, .id = 'id')
asd$src = 'asd'
td  = bind_rows(td_sel, .id = 'id')
td$src  = 'td'
td$lab  = ifelse(grepl('trinity', td$id, fixed = TRUE), 'trinity', 'caltech')
asd$lab = ifelse(grepl('trinity', asd$id, fixed = TRUE), 'trinity', 'caltech')

all = rbind(asd, td)

all[all$lab=='caltech',2:117] = scale(all[all$lab=='caltech',2:117])
all[all$lab=='trinity',2:117] = scale(all[all$lab=='trinity',2:117])


asd = all[all$src == 'asd', 2:117]
td  = all[all$src == 'td', 2:117]
```

## Pooling

We tried pooling data together in different manners, by mean or by median, across subjects or across time.

Pool data together by mean across subjects:
```{r mean per subject, eval=FALSE}
apply_per_subject <- function(df, func, subject_num = 12, jump = 145) {
  df.pool = data.frame()
  for(i in 1:jump) {
    el = apply(df[seq(i, by=jump, length.out=subject_num), ],2, func)
    df.pool = rbind(df.pool, el)
  }
  colnames(df.pool) = colnames(df)
  return(df.pool)
}

asd.pool = apply_per_subject(asd, mean)
td.pool  = apply_per_subject(td, mean)
all.pool = rbind(asd.pool,td.pool)
```

Pool data together by median across subjects:
```{r median per subject, eval=FALSE}
asd.pool = apply_per_subject(asd, median)
td.pool  = apply_per_subject(td, median)
all.pool = rbind(asd.pool,td.pool)
```

Pool data together by mean across time:
```{r mean per time, eval=FALSE}
apply_per_time <- function(df, func, timestamps = 145, subjects_num = 12) {
  df.pool = data.frame()
  for(i in 0:11) {
    el = apply(df[seq(1+i*timestamps, length.out=timestamps), ],2, func)
    df.pool = rbind(df.pool, el)
  }
  colnames(df.pool) = colnames(df)
  return(df.pool)
}

asd.pool = apply_per_time(asd, mean)
td.pool  = apply_per_time(td, mean)
```

Pool data together by median across time:
```{r median per time, eval=FALSE}
asd.pool = apply_per_time(asd, median)
td.pool  = apply_per_time(td, median)
```

We noticed that the pooling that works the best is the one using **median across subjects**, since it's the only one that gives us a decent amount of correlations.
