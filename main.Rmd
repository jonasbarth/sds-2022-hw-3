---
title: "main"
author: "Jonas Barth, Mattia Castaldo, Matteo Migliarini"
date: "2023-02-03"
output: html_document
---

# Index
- [Simulation Study](#simulation-study)
    - [Setup](#parameter-setup)
    - [Creating Data](#creating-data)
    - [Simulation Loop](#simulation-loop)
    - [Different Parameters](#different-parameters)
- [fMRI Data Test](#fmri-data)
    

# Setup
```{r include=FALSE}
install.packages("dplyr")
install.packages("transport")
install.packages("ggplot2")
install.packages('glmnet')
```


```{r setup, include=FALSE}
load("hw3_data.RData")
library('dplyr')
library('transport')
library('ggplot2')
library('glmnet')
```


# Simulation Study {#simulation-study}
Repeat $M$ times:

1. pick two k-variate distributions to sample from. Each distribution represents one class.
1. sample $n_0$ and $n_1$ number of samples from the respective distributions.
1. train a binary classifier
1. do permutation test (slide 8)
Experiment with different:

distributions
sample sizes
Finally, summarise results.

## Setting up parameters {#parameter-setup}
We set up the following parameters for the **simulation study**.

- the simulation size $M$.
- the number of dimensions $k$ for each distribution.
- the number of samples from each distribution, $n0$ and $n1$ respectively.
- the confidence level $\alpha$.
```{r hyperparameters}
M = 1e3
P = 10

k = 5
n0 = 100
n1 = 150

alpha = 0.05
```

## Creating Data {#creating-data}
In this section, we have functions for creating random distributions that will be used in the simulation study. The `random.distro` function let's us draw **univariate** data from a randomly chosen distribution with a determined *mean* and *standard deviation*. To generate a **k-variate** distribution, we call `random.distro` multiple times in the `random.data` function.
```{r}
random.distro <- function(n, mean = 1, sd = 1, noisiness = sd * 0.5, eps = 1e-5) {
  #' Generates a univariate random noisy distribution with n samples.
  #' 
  #' @param n the number of samples to be generated
  #' @param mean the mean of the population to draw samples from
  #' @param sd the standard deviation of the population to draw samples from
  #' @param noisiness the standard deviation of the gaussian noise to be added
  #' @param eps 
  #' 
  #' @return a vector of data sampled from a randomly chosen distribution with the provided mean and standard deviation, and with added noise.
  rd <- sample(1:5, 1)[[1]]

  if (rd==1) {
    rate = 1 / sd
    X = rexp(n,rate) + (mean - rate)
  } else if (rd ==2) {
    X = rnorm(n, mean, sd)
  } else if (rd == 3) {
    shape = (mean/sd)^2
    scale = sd / sqrt(shape)
    X = rgamma(n, shape, scale = scale)
  } else if (rd == 4) {
    sd = max(sqrt(2) + eps, sd)
    df = 2 * sd^2 / (sd^2 - 1)
    X = rt(n, df) + mean
  } else {
    b = mean + sd / sqrt(2)
    a = mean - sd / sqrt(2)
    X =  runif(n, a, b)
  }
  
  noise = rnorm(n, 0, noisiness)
  return(X + noise)
}

random.data <- function(n, k, means = rnorm(k, 0, 3), sds = rnorm(k, 3, 1)) {
  #' Generates a single k-variate random distribution with n samples.
  #' 
  #' @param n the number of samples to generate
  #' @param k the number of dimensions the generated data will have
  #' @param means a vector of k mean values, one per dimension
  #' @param sds a vector of standard deviations, one per dimension
  #' 
  #' @return a dataframe of random data with n rows and k columns.
  cols = list()
  for (i in 1:k) {
    mean = means[i]
    sd = max(1, sds[i])
    cols[[paste(i)]] = random.distro(n, mean, sd)
  }
  return(data.frame(cols))
}

random.data(10,10)
```


```{r}
beg <- Sys.time()
for (i in 1: 100) {
  means = rnorm(7, 0,3)
  sds   = rnorm(7, 3, 1)
  
  X = random.data(450, 7, means = means, sds = sds)
  Y = random.data(450, 7, means = means, sds = sds)
  
  #wasserstein(pp(X), pp(Y))
  subwasserstein(pp(X), pp(Y), 30, 100)
}
fin<- Sys.time() - beg
fin
#sample(X, 10)
```

The `create_df` function builds upon the previous functions data generating functions. It generates a dataframe with data drawn from either two separate distributions, or the same distribution. This function is used in the simulation loop and the data returned from it is used to train the **classifier**. 
```{r}
create_df = function(n0, n1, k, H0 = F, shuffle = T, distance = FALSE) {
    #' Creates a dataframe with samples from the two distributions.
    #' 
    #' @param n0 number of samples from dist0
    #' @param n1 number of samples from dist1
    #' @param k  number of features
    #' @param H0 if TRUE, all samples will come from the same distribution, if FALSE samples will come from two separate distributions.
    #' @param shuffle if TRUE the data will be randomly shuffled, if FALSE data for the two classes will be grouped in the dataframe.
    #' @param distance if TRUE, the Wasserstein distance between the distributions will be added to the list.
    #' 
    #' @return a list with a dataframe with n0 samples from dist0, n1 samples from dist1, and a label column, a train-test split of the data, the means and standard deviations of each of the features.
    
    means = rnorm(k, 0, 10)
    sds   = rnorm(k, 3, 1)
    if (H0) {
      X  = random.data(n0+n1, k, means, sds)
      X0 = X[1:n0, ]
      X1 = X[(n0+1):(n0+n1), ]
    } else {
      X0 = random.data(n0, k, means, sds)
      X1 = random.data(n1, k, means, sds)
    }
    
    y0 = rep(0, n0)
    y1 = rep(1, n1)
    y = rep(c(0, 1), c(n0, n1))
    dim(y) = c(n0 + n1, 1)
    
    n.min = min(n0, n1)
    X0.sample = X0[sample(nrow(X0), n.min), ]
    X1.sample = X1[sample(nrow(X1), n.min), ]

    X = rbind(X0, X1)
    df = data.frame(X)
    df$label = y
    
    if (shuffle)
      df = df[sample(nrow(df), nrow(df)), ]
    
    generated = list()
    generated$df = df
    generated$means = means
    generated$sds = sds
    generated$same.src = H0
    generated$X = X
    generated$y = y
    
    if (distance)
      generated$distance = wasserstein(pp(X0.sample), pp(X1.sample))
    
    return(generated)
}
```



```{r}
train.test.split <- function(df, train.perc = 0.7) {
  #' Splits a dataframe in two 
  #' @param df the dataframe to be splitted
  #' @param train.perc the percentage of rows that will be in the train split
  #' 
  #' @return a list containing **test** and **train** items, each containing a part of the original dataframe
    sample <- sample(c(TRUE, FALSE), nrow(df), replace=TRUE, prob=c(train.perc, 1 - train.perc))
    res = list()
    res$train  <- df[sample, ]
    res$test   <- df[!sample, ]
    return(res)
}

friedman.test <- function(df, alpha = 1) {
  #' Performs a single Friedman test on the dataframe, using ElasticNet regression
  #' 
  #' @param df the dataframe holding the data, the column "label" is expected to hold the target variable
  #' @param alpha same as alpha for `glmnet`
  #' 
  #' @return a list containing:
  #'  - `p.value` : the p-value of the test
  #'  - `model`   : the trained model
    df = train.test.split(df)
  
    L = ncol(df$train)
    X.test = df$test[,1:(L-1)] |> as.matrix()
    X.train = df$train[,1:(L-1)] |> as.matrix()
    y.train = df$train$label 
    
    model = glmnet(X.train, y.train, alpha=alpha, family=binomial)
    y_pred = predict(model, X.test, type='response')

    # get the predicted scores for both classes
    y_pred_class_1 = y_pred[df$test$label == 1]
    y_pred_class_0 = y_pred[df$test$label == 0]
  
    result = list()
    result$p.value = wilcox.test(y_pred_class_0, y_pred_class_1)$p.value
    result$model = model
    return(result)
}
```

## Simulation Loop {#simulation-loop}
The `simulate` function runs $M$ number of simulations to find the **p-value**, **size** and **power** of the **Wilcox** two sample test. In each simulation, we:

1. Create a **k-variate** dataset.
1. Split the dataset into **train** and **test** sets.
1. Train a **Logistic Regression** model on the **train set**.
1. Get the predicted scores of the model on the **test set**.
1. Feed the predicted scores for class 0 and class 1 samples to the **Wilcox** test.
1. Save the **p-value** returned by the **Wilcox** test.

```{r simulation loop}
simulate = function(M, n0, n1, k, create.data, H0) {
    #' Simulates operating characteristics of a testing procedure for two distributions.
    #' 
    #' @param M the number of simulations
    #' @param n0 the number of samples to draw from the first distribution.
    #' @param n1 the number of samples to draw from the second distribution.
    #' @param k the number of features in each distribution.
    #' @param create.data a function for creating a dataframe containing n0 + n1 samples, from the two distributions. The function should take the parameters n0, n1, k, H0.
    #' @return a result vector with p-values from the testing procedure.
    
    results = vector(length = M)
    

    for (i in 1:M) {
        generated = create.data(n0, n1, k, H0 = H0)
        
        res = friedman.test(generated$df)
        
        results[i] = res$p.value
    }
    return(results)
}
```

## Results {#results}
We can observe that when the null hypothesis is false, i.e. when the data comes from two **separate** distributions, then the distribution of p-values is **highly skewed** towards $0$. In particular we can observe that the p-value is **almost always** under the $\alpha = 0.05$ threshold. This happens because the scores generated by the *Logistic Regression* classifier are distributed in a way which makes it highly likely that they come from **separate** distributions.
```{r H1 simulation, echo=FALSE, warning=FALSE}
results.H1 = simulate(M, n0, n1, k, create_df, H0=FALSE)
hist(results.H1, 
     breaks = 20,
     main='Distribution of p-values when H1 is True',
     xlab='p-value')
abline(v=alpha, col = 'orange', lwd=3)
text(alpha + 0.03, 800, labels = 'α', col='orange')
```

We observe that when the null hypothesis is true, i.e. the data comes from the **same** distribution, the distribution of p-values is almost uniform. This happens because the scores generated by the *Logistic Regression* model are very similar for both classes, which means that the model cannot easily distinguish between them and in turn causes p-values to be uniformly distributed when the scores are given to the **Wilcox** test.
```{r H0 simulation, echo=FALSE, warning=FALSE}
results.H0 = simulate(M, n0, n1, k, create_df, H0=TRUE)
hist(results.H0, 
     breaks = 20,
     main='Distribution of p-values when H0 is True',
     xlab='p-value')
abline(v=alpha, col = 'orange', lwd=3)
text(alpha + 0.03, 55, labels= 'α', col='orange')
```

## Experiment with different parameters {#different-parameters}

Now we're going to try the same test but with different parameters:

- the number of features per sample: $k \in \{3, 5, 7\}$
- the number of samples in total: $N \in \{300, 600, 900\}$
- whether the data classes are balance or unbalanced, $p$ being the proportion of one class over the total: $p \in \{20\% , 50\%\}$


```{r experiment new params simulation, eval=FALSE, warning=FALSE}
k.values = seq(3, 7)
N.values = seq(300, 2000, by = 100)
prop.values = c(0.2, 0.5)

total_params = length(k.values) * length(N.values) * length(prop.values)
params.df = expand.grid(k=k.values, N=N.values, prop=prop.values)

performance = data.frame()

for (i in 1:nrow(params.df)) {
  row = params.df[i, ]
  n0 = as.integer(row$N * row$prop)
  n1 = as.integer(row$N - n0)
  
  performance = rbind(
    data.frame(
      N    = rep.int(row$N, M),
      prop = rep(row$prop,  M),
      k    = rep.int(row$k, M),
      size = simulate(M, n0, n1, row$k, create_df, H0=TRUE),
      power= simulate(M, n0, n1, row$k, create_df, H0=FALSE)
    ),
    performance
  )
  write.csv(performance, 'performance.csv', row.names = FALSE) 
}

```

```{r load performance data from disk, include=FALSE}
performance = read.csv('performance.csv')

k.values = sort(unique(performance$k))
N.values = sort(unique(performance$N))
prop.values = sort(unique(performance$prop))
params.df = expand.grid(k=k.values, N=N.values, prop=prop.values)
```

The plot below shows the **size** of the test, i.e. the probability of observing a **Type 1 Error** (false positive) as a function of the **number of features** $k$, and the **number of samples**. Each circle is the **average p-value** of $M$ simulations. We see that the probability of a false discovery, i.e. thinking that the data does **NOT** come from two **separate distributions** when it actually does is within a range of $0.035$ and $0.07$. The probabilities are mostly the same, irregardless of the of the number of features, number of samples, and class imbalances.
```{r}
performance$prop = ifelse(performance$prop == 0.2, 'Imbalanced 20-80', 'Balanced 50-50')

performance.summary = performance |>
  group_by(k, N, prop) |>
  dplyr::summarise(
    type.one = sum(size < alpha) / length(size),
    type.two = 1 - sum(power > alpha) / length(power), 
    .groups = 'keep'
  )

performance.summary |>
  ggplot(
    aes(x=k, y = type.one)
  ) +
  geom_boxplot(aes(group=interaction(k, prop), col=prop)) +
  geom_dotplot(aes(group=k+N, fill=N), binaxis='y', stackdir='center', dotsize=.2, binwidth = 0.005) +
  ggtitle('Distribution of Type I error probability') +
  xlab('Number of features [k]') +
  ylab('P(Type I error)') + 
  labs(fill = 'Number of samples', col = 'Class balance')
```

The plot below shows the **power** of the test, i.e. observing a **true positive** (complement of false negative) as a function of the **number of features** $k$, and the **number of samples**. Again, each circle is the **average p-value** of $M$ simulations. Here we can see that the probability of correctly rejecting the null hypothesis when it is false increases with the number of features and somewhat with the number of samples. For $k=3$, the probability of a **true discovery** ranges from $0.75$ to $0.85$, however for $k=7$ it is between $0.95$ and $1.0$, indicating a positive relationship between the two variables. This could be explained by the fact that as the number of dimensions increase, the amount of useful information encoded in them which helps the Logistic Regression model separate the classes also increases. Additionally, we also observe that **p-values** for smaller samples as indicated by **darker** circles cluster more towards the lower end of the probability interval for a specific value of $k$. E.g. for $k=6$, darker circles are more concentrated towards the bottom.
```{r}
performance.summary |>
  ggplot(
    aes(x=k, y = type.two)
  ) +
  geom_boxplot(aes(group=interaction(k, prop), col=prop)) +
  geom_dotplot(aes(group=k+N, fill=N), binaxis='y', stackdir='center', dotsize=.5, binwidth = 0.01) +
  ggtitle('Distribution of Type II error probability') +
  xlab('Number of features [k]') +
  ylab('1 - P(Type II error)') + 
  labs(fill = 'Number of samples', col = 'Class balance')
```



# 4. {#fmri-data}

## Feature Engineering {#feature-engineering}
```{r}
#' Applies a certain function to all the time-series columns in a df
apply_per_time <- function(data, func) 
  data |>
    lapply(function(patient) sapply(patient, func)) |> 
    data.frame() |>
    t() |>
    data.frame() 

extract_feat_ts <- function(data, ...)  {
  #' Applies summaries to the data. 
  #' @param data a list holding N k-variate time series
  #' @param ...  a number of M functions that take in input an univariate time-series and give in in output a single value
  #' 
  #' @return a dataframe of with $N$ rows and $M*k$ columns, one for each function applied to a single time-series
  func.list = list(...)
  
  func.list |>
    lapply(function(func) apply_per_time(data, func)) %>%
    do.call(cbind.data.frame, . )
  }

```

```{r}
jump <- function(x, func=mean) {
  #' Applies a function to the difference of each element with the next one 
  #' @param x an array of data
  #' @param func the function that should be applied
  L = length(x)
  
  func(x[2:L] - x[1:(L-1)])
}
```

```{r}
#' Extracts a certain number of feature from the ASD or TD dataset
ts.feature.engine = function(data, label=1) data %>%
  extract_feat_ts(
    mean   = mean,
    median = median,
    max    = max,
    min    = min,
    sd     = sd,
    perc.95= . %>% quantile(.95),
    perc.05= . %>% quantile(.05),
    jump.sd    = . %>% jump(sd),
    jump.avg   = . %>% jump(mean),
    jump.median= . %>% jump(median)
  ) |>
  cbind(label=label)

asd.ts.feat = ts.feature.engine(asd_data, label=1)
td.ts.feat  = ts.feature.engine(td_data,  label=0)
ts.feat = rbind(asd.ts.feat, td.ts.feat)
ts.feat = ts.feat[-ncol(ts.feat)] %>%
  scale() %>%
  as.data.frame() %>%
  cbind(ts.feat[ncol(ts.feat)])
```

## Simulation
We run the Friedman test on the engineered fMRI data for $M = 100$ times to get a range of p-values. The two stochastic sources in the simulation are the **train-test** split of the data as well as the regularisation parameter $\lambda$ used by the `glmnet` package.
```{r}
M = 1e2
feature.importances = data.frame()
p.values = numeric

for (i in 1:M) {
  f.test = friedman.test(ts.feat)
  p.values[i] = f.test$p.value
  theta = coef(f.test$model) |> as.matrix() |> as.data.frame()
  feature.importances = rbind(feature.importances, theta[, ncol(theta)])
}

colnames(feature.importances) = row.names(theta)
```

The histogram below shows the frequency of p-values from the simulations. We can see that most p-values are **below** the threshold $\alpha = 0.05$, which means that our procedure says that the TD and ASD data come from **different** distributions most of the time. Therefore, we cannot exclude that they come from different distributions. 
```{r}
hist(p.values, breaks = 50,
     main = paste("p-values for fMRI Data, M =", M),
     xlab = "p-value",
     col = "orchid",
     border = "white")
abline(v=alpha, col = 'orange', lwd=3)
text(alpha + 0.03, 55, labels= 'α', col='orange')
```


```{r}
feature.means = lapply(feature.importances, mean) |> as.data.frame() |> t() |> as.data.frame()
feature.means$feature_name = row.names(feature.means)
threshold = feature.means[[1]] %>% abs() %>% quantile(0.95)
top.10 = feature.means[feature.means[[1]] > threshold, ]
```


Jumps seem more important for identifying ASD subjects.
```{r}
top.10 |>
  ggplot(aes(
    x = reorder(feature_name, abs(top.10[[1]])), 
    y = top.10[[1]],
    fill = ifelse(sign(top.10[[1]]) == 1, 'TD', 'ASD'))) +
  geom_col() +
  coord_flip() +
  labs(
    x = '',
    y = 'Weight',
    title = 'Feature importance',
    fill  = ''
  )
```

As lambda grows, our coefficients converge to 0.
```{r}
plot(f.test$model, xvar='lambda')
```
